# Алгоритмы сортировки
by Юманов Василий

# Теория

#### Алгоритмы сортировки обладают следующими свойствами:

- [**Устойчивость**](https://ru.wikipedia.org/wiki/Устойчивая_сортировка) — не меняет взаимного расположения элементов с одинаковыми ключами.

- **Естественность поведения** — эффективность метода при обработке уже упорядоченных или частично упорядоченных данных. Алгоритм ведёт себя естественно, если учитывает эту характеристику входной последовательности и работает лучше.

- **Использование операции сравнения**. Алгоритмы, использующие для сортировки сравнение элементов между собой, называются основанными на сравнениях. Минимальная трудоёмкость *худшего случая* для этих алгоритмов составляет $O(n \log n)$, но они отличаются гибкостью применения. Для специальных случаев (типов данных) существуют более эффективные алгоритмы.

Ещё одним важным свойством алгоритма является его сфера применения. Здесь основных типов упорядочения два:

- [**Внутренняя сортировка**](https://ru.wikipedia.org/wiki/Внутренняя_сортировка) оперирует массивами, целиком помещающимися в оперативной памяти с произвольным доступом к любой ячейке. Данные обычно упорядочиваются на том же месте без дополнительных затрат.
    - В современных архитектурах персональных компьютеров широко применяется подкачка и кэширование памяти. Алгоритм сортировки должен хорошо сочетаться с применяемыми алгоритмами кэширования и подкачки.

- [**Внешняя сортировка**](https://ru.wikipedia.org/wiki/Внутренняя_сортировка) оперирует запоминающими устройствами большого объёма, но не с произвольным доступом, а последовательным, то есть в данный момент «виден» только один элемент, а затраты на перемотку по сравнению с памятью неоправданно велики. Это накладывает некоторые дополнительные ограничения на алгоритм и приводит к специальным методам упорядочения, обычно использующим дополнительное дисковое пространство. Кроме того, доступ к данным во внешней памяти производится намного медленнее, чем операции с оперативной памятью.
    - Доступ к носителю осуществляется последовательным образом: в каждый момент времени можно считать или записать только элемент, следующий за текущим.
    - Объём данных не позволяет им разместиться в ОЗУ.

#### Эффективность алгоритма определяется:
- Временем работы
- Объемом дополнительно используемой памяти
- Другими характеристики. Например, количеством операций сравнения или количеством обращений к диску.

#### Ассимптотические обозначения [вычислительной сложности](https://ru.wikipedia.org/wiki/Вычислительная_сложность)
**Опр.** Пусть $f$, $g$ - функции $\mathbb N \rightarrow \mathbb N$. $\mathbb N = \{1,2,3, ...\}$. Тогда:

$f$ ограничена сверху функцией $g$ (с точностью до постоянного множителя) асимптотически

$f = O(g)$, если $\exists C,N_0: \forall n \in \mathbb N, n \geq N_0 \hookrightarrow f(n) \leq C*g(n)$

$f$ ограничена снизу функцией $g$ (с точностью до постоянного множителя) асимптотически

$f = \Omega (g)$, если $\exists C,: \forall n \in \mathbb N \hookrightarrow f(n) \geq C*g(n)$

$f$ ограничена снизу и сверху функцией $g$ (с точностью до постоянного множителя) асимптотически

$f = \Theta (g)$, если $\exists C_1, C_2, N_0: \forall n \in \mathbb N, n \geq N_0 \hookrightarrow C_1 * g(n) \leq f(n) \leq C_2 * g(n)$

# Рассмотрены следующие алгоритмы сортировки:
Устойчивые алгоритмы:
1. Bubble Sort
1. Cocktail Sort
1. [Insertion Sort](#insertion-sort---сортировка-вставками)
1. Gnome Sort
1. Merge Sort
1. Tree sort
1. Tim Sort

Неустойчивой алгоритмы:
1. Selection Sort
1. Comb Sort
1. Shell sort
1. Heap Sort
1. Smooth Sort
1. [Quick Sort](#quick-sort---быстрая-сортировка)
1. Intro sort
1. Stooge sort

Алгоритмы, не основывающиеся на сравнениях:
1. [Bucket sort](#bucket-sort---сортировка-сегментами)
1. [Radix sort](#radix-sort---поразрядная-сортировка)
1. [Burstsort](#burstsort)
1. [Counting sort](#counting-sort---сортировка-подсчетом)

Непрактичные алгоритмы:
1. Bogosort
2. Bead sort

(Пока не сгруппированные)
1. Std::sort (gcc)
1. Block Merge Sort (wiki sort)
1. Bitonic Sort
1. Binary Insertion Sort
1. Cycle Sort
1. Odd-Even Sort

Алгоритмы рассматривались по следующему шаблону:

1. Краткое описание алгоритма, его вариации, преимущества и недостатки
1. Суть алгоритма
1. Псевдокод
1. Анализ сложности (Объём дополнительной используемой памяти и время работы в худшем и среднем случае)
1. Способы оптимизации алгоритма (опционально)
1. Сравнение с другими алгоритмами (опционально)

# О каждом из них в подробностях

## Insertion sort - [сортировка вставками](https://en.wikipedia.org/wiki/Insertion_sort)
Базовый алгоритм, не самый эффективный для сортировки больших массивов данных, но является одним из самых быстрых алгоритмов для сортировки маленьких массивов (даже быстрее [quicksort](#quick-sort---быстрая-сортировка)'a!), что делает его довольно полезным при использовании в других алгоритмах, например, в [Bucket sort](#bucket-sort---сортировка-сегментами).

### Алгоритм:
На каждом шаге выбирается один из элементов входных данных и помещается на нужную позицию в уже отсортированной последовательности до тех пор, пока набор входных данных не будет исчерпан.

![Insertion_Sort_1](Images/Insertion%20Sort/Insertion_Sort_1.gif)

### Псевдокод
```
for j = 2 to A.length do 
    key = A[j]
    i = j-1
    while (i >= 0 and A[i] > key) do 
        A[i + 1] = A[i]
        i = i - 1
    end while
    A[i+1] = key
end
```

### Анализ сложности
Требуется $O(n)$ памяти в худшем случае, $O(1)$ вспомогательно.

Для каждой инструкции алгоритма введём временную стоимость и количество повторений, где $t_j$ — количество проверок условия во внутреннем цикле while:

| Код                        | Стоимость | Повторы               |
| -------------------------- | --------- | --------------------- |
| for j = 2 to A.length      | $c_1$     | $n$                   |
| key = A[j]                 | $c_2$     | $n-1$                 |
| i = j - 1                  | $c_3$     | $n-1$                 |
| while i > 0 and A[i] > key | $c_4$     | $\sum_{j=2}^{n}t_j$   |
| A[i+1] = A[i]              | $c_5$     | $\sum_{j=2}^{n}t_j-1$ |
| i = i - 1                  | $c_6$     | $\sum_{j=2}^{n}t_j-1$ |
| A[i+1] = key               | $c_7$     | $n-1$                 |

#### Худший случай
Наихудшим случаем является массив, отсортированный в обратном порядке. При этом каждый новый элемент сравнивается со всеми в отсортированной последовательности. Это означает, что все внутренние циклы состоят из j итераций, то есть $t_j = j$ для всех $j$. Тогда время работы алгоритма составит:
$$
T(n) = c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \sum_{j=2}^{n} j + c_5 \sum_{j=2}^{n} (j-1) + c_6 \sum_{j=2}^{n} (j-1) + c_7 (n-1) = O(n^2)
$$

#### Средний случай
Для анализа среднего случая нужно посчитать среднее число сравнений, необходимых для определения положения очередного элемента. При добавлении нового элемента потребуется, как минимум, одно сравнение, даже если этот элемент оказался в правильной позиции. $i$-й добавляемый элемент может занимать одно из $i+1$ положений. Предполагая случайные входные данные, новый элемент равновероятно может оказаться в любой позиции. Среднее число сравнений для вставки $i$-го элемента:
$$
T_i = {1 \over i+1} \bigg(\sum_{p=1}^{i} p + i \bigg) = {1 \over i+1} \bigg({i(i+1) \over 2} + i \bigg) = {i \over 2} + 1 - {1 \over i+1}
$$
Для оценки среднего времени работы для $n$ эелментов нужно просуммировать:
$$
T(n) = \sum_{i=1}^{n-1} T_i = \sum_{i=1}^{n-1} \bigg({i \over 2} + 1 - {1 \over i+1} \bigg) = \sum_{i=1}^{n-1} {i \over 2} + \sum_{i=1}^{n-1} 1 + \sum_{i=1}^{n-1} {1 \over i+1}
$$

$$
T(n) \approx {n^2-n \over 4} + (n-1) - (\ln(n)-1) = O(n^2)
$$

---

## Quick sort - [быстрая сортировка](https://en.wikipedia.org/wiki/Quicksort)

---

## Bucket sort - [сортировка сегментами](https://en.wikipedia.org/wiki/Bucket_sort#)
Сортировка предполагает, что исходные данные распределены по известному [распределению](https://ru.wikipedia.org/wiki/Распределение_вероятностей)

### Вариации
1) Proxmap sort
2) Histogram sort
3) Postman's sort
4) Shuffle sort

### Алгоритм: 
1) **Разбиваем** отрезок $[min, max]$ на $n$ одинаковых сегментов;
2) **Рассеиваем** исходные данные по соответствующим им сегментам;
3) **Сортируем** каждый непустой сегмент;
4) **Объединям** все сегменты по их порядку на отрезке.

![Bucket_Sort_1](Images/Bucket%20Sort/Bucket_sort_1.svg)
![Bucket_Sort_2](Images/Bucket%20Sort/Bucket_sort_2.svg)

### Псевдокод
```
function bucket-sort(A, n) is
  buckets ← новый массив из n пустых элементов
  for i = 0 to (length(A)-1) do
    вставить A[i] в конец массива buckets[msbits(A[i], k)]
  for i = 0 to n - 1 do
    next-sort(buckets[i])
  return Конкатенация массивов buckets[0], ..., buckets[n-1]
```

### Анализ сложности:
В худшем случае требуется $O(n+k)$ памяти, где $k$ - количество сегментов.
#### Худший случай:
Когда входные данные содержат несколько элементов, расположенных близко друг к другу, эти элементы, скорее всего, будут помещены в один и тот же сегмент, в результате чего некоторые сегменты будут содержать больше элементов, чем в среднем. Наихудший сценарий возникает, когда все элементы помещаются в один сегмент.
Тогда общая производительность будет зависеть от алгоритма, используемого для сортировки каждого сегмента, например $O(n^2)$ для сортировки вставкой или $O(nlogn)$ для сортировки сравнением.

#### Средний случай:
Рассмотрим случай, когда входные данные распределены равномерно.

Для оценки введём случайную величину $n_i$, обозначающую количество элементов в сегменте $B[i]$. Время работы сортировки вставками равно
$O(n^2)$.

Время работы алгоритма: $T(n)=\Theta(n) + \sum_{i = 0}^{n-1}O(n_i^2)$

Вычислим математическое ожидание обеих частей равенства:

$$
M(T(n))
= M\bigg(\Theta(n)+\sum_{i = 0}^{n-1}O(n_i^2)\bigg)
= \Theta(n) + \sum_{i=0}^{n-1}O(M(n_i^2))
$$

Найдём величину $M(n_i^2)$.
Введём случайную величину $X_{ij}$, которая равна 1, если $A[j]$ попадёт в $i$-й сегмент, и 0 в противном случае: $n_i = \sum_{j = 1}^nX_{ij}$

$$
M(n_i^2)
= M\bigg[(\sum_{j=1}^n X_{ij})^2\bigg]
= M\bigg[\sum_{j=1}^n \sum_{k=1}^n X_{ij} X_{ik}\bigg]
= \sum_{j=1}^n M\bigg[X_{ig}^2\bigg] + \sum_{1 \leq j \leq n} \sum_{1 \leq k \leq n, k \neq j} M[X_{ij} X_{ik}]
$$
$$
M\big[X_{ij}^2\big] = 1 \cdot {1 \over n} + 0 \cdot \big (1 - {1 \over n}\big) = {1 \over n}
$$

Если $k \neq j$, величины $X_{ij}$ и $X_{ik}$ независимы, поэтому:
$M[X_{ij} X_{ik}] = M[X_{ij}] M[X_{ik}] = {1 \over n^2}$

Таким образом
$$
M(n_i^2) = \sum_{j=1}^n {1 \over n} + \sum_{1 \leq j \leq n} \sum_{1 \leq k \leq n, k \neq n} {1 \over n^2} = 2 - {1 \over n}
$$

Итак, ожидаемое время работы алгоритма равно
$\Theta(n) = n \cdot O(2-{1 \over n}) = \Theta (n)$

Или же $O \bigg(n + {n^2 \over k} + k \bigg)$, где $k = \Theta (n)$

### Оптимизация
Оптимизация заключается в том, чтобы *сначала* поместить неотсортированные элементы сегментов обратно в исходный массив, а затем сортировать вставкой по всему массиву; поскольку время выполнения сортировки вставками зависит от того, насколько далеко каждый элемент находится от своей конечной позиции, количество сравнений остается относительно небольшим.

Если входное распределение известно или может быть оценено, часто можно выбрать разделение отрезка на сегменты с одинаковой плотностью распределения. Это позволяет среднеюю сложность $O(n)$ даже без равномерно распределенных входных данных.

### Сравнение с другими алгоритмами
Сортировку сегментов можно рассматривать как обобщение сортировки подсчётом; Фактически, если размер каждого сегмента равен 1, сортировка сегментов вырождается в сортировку с подсчётом. Переменный размер сегмента сортировки сегментов позволяет использовать память $O(n)$ вместо памяти $O(k)$, где $k$ — количество различных значений; взамен он отказывается от подсчета худшего поведения сортировки $O(n+k)$.

Сортировка сегментами с двумя сегментами по сути является версией быстрой сортировки, в которой опорное значение всегда выбирается как среднее значение диапазона значений. Хотя этот выбор эффективен для равномерно распределенных входных данных, другие способы выбора опорной точки при быстрой сортировке, такие как случайно выбранные опорные точки, делают ее более устойчивой к кластеризации во входном распределении.

Алгоритм n-сторонней сортировки слиянием также начинается с разделения массива на n подмассивов и сортировки каждого из них; однако подмассивы, созданные сортировкой слиянием, имеют перекрывающиеся диапазоны значений, поэтому их нельзя повторно объединить путем простой конкатенации, как при сортировке сегментов. Вместо этого они должны чередоваться с помощью алгоритма слияния. Однако эти дополнительные расходы уравновешиваются более простой фазой рассеивания и возможностью гарантировать, что каждый подсписок имеет одинаковый размер.
обеспечивая хорошее время для наихудшего случая.

Поразрядную сортировку сверху вниз можно рассматривать как особый случай сортировки по сегментам, где диапазон значений и количество сегментов ограничены степенью двойки. Следовательно, размер каждого сегмента также равен степени двойки, и процедуру можно применять рекурсивно. Этот подход может ускорить фазу рассеяния, поскольку нам нужно проверить только префикс битового представления каждого элемента, чтобы определить его сегмент.

---

## Radix sort - [поразрядная сортировка](https://en.wikipedia.org/wiki/Radix_sort)

### Вариации:
1) LSD (*least significant digit*)
2) MSD (*most significant digit*)

При LSD-сортировке получается порядок, уместный для чисел. Например: 1, 2, 9, 10, 21, 100, 200, 201, 202, 210. То есть, значения сначала сортируются по единицам, затем сортируются по десяткам, сохраняя отсортированность по единицам внутри десятков, затем по сотням, сохраняя отсортированность по десяткам и единицам внутри сотен, и так далее.

При MSD-сортировке получается алфавитный порядок, уместный для сортировки строк текста. Например "b, c, d, e, f, ba" отсортируется как "b, ba, c, d, e, f". Если MSD применить к числам, то получится алфавитный, но не числовой порядок: 1, 10, 100, 2, 200, 201, 202, 21, 210, 9.

### Алгоритм
Сравнение производится поразрядно: сначала сравниваются значения одного крайнего разряда, и элементы группируются по результатам этого сравнения, затем сравниваются значения следующего разряда, соседнего, и элементы либо упорядочиваются по результатам сравнения значений этого разряда внутри образованных на предыдущем проходе групп, либо переупорядочиваются в целом, но сохраняя относительный порядок, достигнутый при предыдущей сортировке. Затем аналогично делается для следующего разряда, и так до конца.

### Псевдокод
```
function radix-sort(Arr, n)
    int m = max(Arr) ← наибольшее число, чтобы знать, по скольки разрядам производить сравнение
    for (int exp = 1; m / exp > 0; exp *= 10)
        counting-sort(arr, n, exp);
```
где counting-sort - [сортирвка подсчётом](#counting-sort---сортировка-подсчётом)

### Анализ сложности
$\sqsupset \omega$ - количество бит, требуемых для хранения каждого ключа

Сортировка требует дополнительного места для сегментов, используемых во время сортировки, и для хранения отсортированных результатов. Итого $O(n+\omega)$ памяти. Объём дополнительной памяти может быть выше при работе с большим диапазоном входных значений.

#### Средний случай
Сортировка обрабатывает каждую цифру, каждого элемента входного массива 1 раз, получая сложность $O(n \cdot \omega)$. LSD варианты могут достичь нижней границы "средней длины входных данных", разделяя их на группы с одинаковыми длинами, как описано в [сортировке сегментами](#bucket-sort---сортировка-сегментами).

#### Худший случай
В худшем случае, когда все элементы имеют одинаковые цифры или цифры расположены в обратном порядке, поразрядной сортировке все равно необходимо просто обрабатывать каждую цифру каждого элемента. И итоговая сложность $O(n \cdot \omega)$.

### Оптимизации:
1. Оптимизации "на месте"
1. Оптимизации стабильности
1. Гибридные подходы
1. Распараллеливание вычислений
1. Построение дерева

#### Оптимизации "на месте"
Бинарная MSD поразрядная сортировка, также известная как бинарная быстрая сортировка может быть реализована разделением на две ячейки: нулей и единиц. Ячейкa нулей растёт с начала массива, а ячейкa единиц - с конца, их границы помещаются перед первым и после последнего элемента соответственно. Просматривается первый разряд первого элемента массива. Если это 0, то он остаётся на месте, а граница нулей увеличивается на 1. Если же это 1, то он меняется местами с элементом, стоящим перед границей единиц, которая увеличивается на 1. Далее проверяется элемент, стоящий перед границей нулей. Этот процесс продолжается до тех пор, пока ячейки нулей и единиц не достигнут друг друга. Таким образом рекурсивно обрабатываются все остальные значащие биты.

Также бинарная поразрядная сортировка может быть расширена до бóльших разрядностей и сохранить возможность сортировки "на месте". Используя [сортировку подсчётом](#counting-sort---сортировка-подсчётом) можно определить размер каждой ячейки и их начальный индекс. Перестановками помещается текущий элемент в соответствующую ему ячейку с последующим расширением ячейки. При проверке элементов массива ячейки пропускаются и обрабатываются только элементы между ячейкaми, пока не будет обработан весь массив и все элементы не окажутся в соответствующих ячейкaх. Количество ячеек совпадает с используемой системой счисления - например, 16 ячеек для 16-й системы счисления. Каждый проход основан на одной цифре (например, 4 бита на цифру в случае 16-разрядного основания), начиная со старшей цифры. Затем каждая ячейка обрабатывается рекурсивно с использованием следующей цифры, пока все цифры не будут использованы для сортировки.

Ни поразрядная сортировка, ни её бинарная вариации не являются стабильными.

#### Оптимизации стабильности
Поразрядная сортировка может быть реализована как стабильный алгоритм, но требует использования буфера памяти того же размера, что и входной массив. Эта память позволяет проверять входной массив от первого элемента до последнего и помещать их в соответствующие ячейки в том же порядке. Сортировка по следующему разряду использует память, затраченную исходным массивом, для записи своего результата. Каждая из ячеек обрабатывается рекурсивно, как это делается для поразрядной сортировки "на месте". После завершения сортировки по последней цифре проверяется выходной буфер, чтобы определить, является ли он исходным входным массивом, и если это не так, то выполняется однократное копирование. Если размер цифры выбран таким образом, чтобы размер ключа, деленный на размер цифры, был четным числом, то можно избежать копирования в конце.

#### Гибридные подходы
Поразрядная сортировка, такая как двухпроходный метод, при котором на первом этапе каждого уровня рекурсии используется [сортировка подсчётом](#counting-sort---сортировка-подсчётом), требует больших постоянных затрат. Таким образом, когда ячейки становятся маленькими, следует использовать другие алгоритмы сортировки, такие как [сортировка вставками](#insertion-sort---сортировка-вставками). Хорошая реализация сортировки вставкой быстра для небольших массивов, стабильна, работает на месте и может значительно ускорить сортировку по основанию.

#### Распараллеливание вычислений
Каждая ячейка может быть отсортирована независимо, что позволяет разделить вычисления на несколько потоков. В этом случае каждая ячейка передается следующему доступному процессору. В начале будет использоваться один процессор (старший разряд). Ко второму или третьему разряду, скорее всего, будут задействованы все доступные процессоры. В идеале, по мере того как каждый разряд будет полностью отсортирован, будет использоваться все меньше и меньше процессоров. В худшем случае все ключи будут идентичны или почти идентичны друг другу, в результате чего использование параллельных вычислений для сортировки ключей практически не даст никаких преимуществ.

На верхнем уровне рекурсии возможность параллелизма заложена в части алгоритма [сортировки подсчётом](#counting-sort---сортировка-подсчётом). Подсчет в высшей степени параллелен, подчиняется шаблону parallel_reduce и эффективно распределяет работу между несколькими ядрами, пока не будет достигнут предел пропускной способности памяти. В этой части алгоритма параллелизм не зависит от данных. Однако обработка каждой ячейки на последующих уровнях рекурсии зависит от данных. Например, если бы все ключи имели одинаковое значение, то существовала бы только одна ячейка с любыми элементами в ней, и параллелизм был бы невозможен. При случайном вводе данных все ячейки были бы заполнены почти одинаково, и было бы доступно большое количество возможностей для параллелизма.

#### Построение дерева
Поразрядная сортировка может быть реализована путём построения дерева (или же [сжатого префиксного дерева](https://ru.wikipedia.org/wiki/Сжатое_префиксное_дерево)) из исходных данных и выполнения предварительного обхода. Это может быть полезно при работе с определенными типами данных, например в [burstsort](#burstsort)'е

---

## [Burstsort](https://en.wikipedia.org/wiki/Burstsort)

Burstsort и его вариации - это алгоритмы сортировки строк с использованием кэш-памяти. Они являются вариантами [поразрядной сортировки](#radix-sort---поразрядная-сортировка), но более быстрыми для больших наборов данных, состоящих из общих строк.

### Алгоритм
Burstsort использует [префиксное дерево](https://ru.wikipedia.org/wiki/Префиксное_дерево) для хранения префиксов строк, а в качестве конечных узлов используются расширяемые массивы указателей, содержащие отсортированные уникальные суффиксы (называемые сегментами). Когда количество сегментов превышает заданный порог, они "разбиваются" на префиксные поддеревья. В большинстве реализаций для сортировки содержимого корзин используется [быстрая сортировка](#quick-sort---быстрая-сортировка) по нескольким ключам, являющаяся расширением трехпозиционной быстрой сортировки по основанию. Разделив входные данные на группы с общими префиксами, можно выполнить сортировку с использованием эффективного кэширования.

### Анализ сложности

Burstsort по сути всё ещё является поразрядной MSD сортировкой, но более быстрой из-за того, что кэширование и связанные с ним основания хранятся ближе друг к другу из-за особенностей структуры префиксного дерева. Она использует особенности строк, которые обычно встречаются в реальном мире. И хотя асимптотически это то же самое, что и поразрядная сортирвока, с временной сложностью $O(\omega n)$ ($\omega$ – длина слова и $n$ – количество строк, подлежащих сортировке), но из-за лучшего распределения памяти она, как правило, выполняется в два раза быстрее при больших наборах данных.

### Псевдокод
```
function burst-sort(A):
    if length(A) == 1:
        return A
    // В данном варианте массив делится ровно пополам, но существуют и другие эффективные разбиения
    subarr1, subarr2 = A[start:len(A)//2], A[len(A)//2:end]
 
    arr1 = burstSort(subarr1)
    arr2 = burstSort(subarr2)
    // Соединение массивов с одновременной сортировкой
    return merge(arr1, arr2)
```

---

## Counting sort - [сортировка подсчётом](https://en.wikipedia.org/wiki/Counting_sort)

Сортировка подсчётом - алгоритм сортировки, в котором используется диапазон чисел сортируемого массива для подсчёта совпадающих элементов. Применение сортировки подсчётом целесообразно лишь тогда, когда сортируемые элементы имеют диапазон значений, который достаточно мал по сравнению с сортируемым множеством, например, миллион натуральных чисел меньших 1000.
Предположим, что исходный массив состоит из $n$ целых чисел в диапазоне от $0$ до $k-1$.

### Рассмотренные вариации:
1. Простой алгоритм
1. Алгоритм со списком
1. Устойчивый алгоритм

#### Простой алгоритм
Это простейший вариант алгоритма. Создать вспомогательный массив `C[0..k]`, состоящий из нулей, затем последовательно прочитать элементы входного массива `A`, для каждого `A[i]` увеличить `C[A[i]]` на единицу. Теперь достаточно пройти по массиву `C`, для каждого $j \in \{0,...,k \}$ в массив `A` последовательно записать число $j$ `C[j]` раз.
##### Псевдокод
```
simple-counting-sort:
    for i = 0 to k
        C[i] = 0;
    for i = 0 to n - 1
        C[A[i]] = C[A[i]] + 1;
    b = 0;
    for j = 0 to k
        for i = 0 to C[j] - 1
            A[b] = j;
            b = b + 1;
```

#### Алгоритм со списком
Этот вариант используется, когда на вход подается массив структур данных, который следует отсортировать по ключам. Нужно создать вспомогательный массив `C[0..k - 1]`, каждый `C[i]` в дальнейшем будет содержать список элементов из входного массива. Затем последовательно прочитать элементы входного массива `A`, каждый `A[i]` добавить в список `C[A[i].key]`. В заключении пройти по массиву C, для каждого $j \in \{0,...,k-1 \}$ в массив `A` последовательно записывать элементы списка `C[j]`. Алгоритм устойчив.
##### Псевдокод
```
list-counting-sort
    for i = 0 to k - 1
        C[i] = NULL;
    for i = 0 to n - 1
        C[A[i].key].add(A[i]);
    b = 0;
    for j = 0 to k - 1
        p = C[j];
        while p != NULL
            A[b] = p.data;
            p = p.next();
            b = b + 1;
```

#### Устойчивый алгоритм
В этом варианте помимо входного массива A потребуется два вспомогательных массива — `C[0..k - 1]` для счётчика и `B[0..n - 1]` для отсортированного массива. Сначала следует заполнить массив `C` нулями, и для каждого `A[i]` увеличить `C[A[i]]` на 1. Далее подсчитывается количество элементов меньших или равных $k-1$. Для этого каждый `C[j]`, начиная с `C[1]`, увеличивают на `C[j - 1]`. Таким образом в последней ячейке будет находиться количество элементов от $0$ до $k-1$ существующих во входном массиве. На последнем шаге алгоритма читается входной массив с конца, значение `C[A[i]]` уменьшается на 1 и в каждый `B[C[A[i]]]` записывается `A[i]`. Алгоритм устойчив.
##### Псевдокод
```
stable-counting-sort
    for i = 0 to k - 1
        C[i] = 0;
    for i = 0 to n - 1
        C[A[i]] = C[A[i]] + 1;
    for j = 1 to k - 1
        C[j] = C[j] + C[j - 1];
    for i = n - 1 to 0
        C[A[i]] = C[A[i]] - 1;
        B[C[A[i]]] = A[i];
```

### Анализ сложности
Используемая память в первых двух алгоритмах равна $\Theta (k)$, а в третьем $\Theta (n+k)$.

В первых двух алгоритмах первые два цикла работают за $\Theta (k)$ и $\Theta (n)$ соответственно, двойной цикл за $\Theta (n+k)$. В третьем алгоритме циклы занимают $\Theta (k)$, $\Theta (n)$, $\Theta (k)$ и $\Theta (n)$ соответственно. Итого все три алгоритма имеют линейную сложность $\Theta (n+k)$.

---
